{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "tFM0tszqGwTG"
      },
      "id": "tFM0tszqGwTG"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "This cell imports all the required libraries for dataset handling, preprocessing, and downloading.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "reWvVfWOGrt4"
      },
      "id": "reWvVfWOGrt4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Load MedQA-USMLE dataset"
      ],
      "metadata": {
        "id": "285kI7QpGzxX"
      },
      "id": "285kI7QpGzxX"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Loads the MedQA-USMLE dataset and saves train and test splits to the project directory.\n",
        "\"\"\"\n",
        "dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\")\n",
        "base_dir = \"NLP_Project/MedQA_USMLE\"\n",
        "os.makedirs(f\"{base_dir}/train\", exist_ok=True)\n",
        "os.makedirs(f\"{base_dir}/test\", exist_ok=True)\n",
        "dataset[\"train\"].to_csv(f\"{base_dir}/train/medqa_usmle_train.csv\")\n",
        "dataset[\"test\"].to_csv(f\"{base_dir}/test/medqa_usmle_test.csv\")\n"
      ],
      "metadata": {
        "id": "fTsIZGMvGyJX"
      },
      "id": "fTsIZGMvGyJX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download files from PubMedQA repository"
      ],
      "metadata": {
        "id": "wiLBqNIpG6oF"
      },
      "id": "wiLBqNIpG6oF"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Fetches data files from the PubMedQA GitHub repository and saves them to the specified directory.\n",
        "\"\"\"\n",
        "repo_url = \"https://api.github.com/repos/pubmedqa/pubmedqa/contents/data\"\n",
        "output_dir = \"NLP_Project/Pubmedqa_data/data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "response = requests.get(repo_url)\n",
        "if response.status_code == 200:\n",
        "    for file_info in response.json():\n",
        "        file_url = file_info[\"download_url\"]\n",
        "        file_name = file_info[\"name\"]\n",
        "        with open(f\"{output_dir}/{file_name}\", 'wb') as f:\n",
        "            f.write(requests.get(file_url).content)\n",
        "    print(\"All files downloaded successfully!\")\n",
        "else:\n",
        "    print(\"Failed to fetch file list.\")\n"
      ],
      "metadata": {
        "id": "5Ho_542mG4cq"
      },
      "id": "5Ho_542mG4cq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Merge PubMedQA datasets"
      ],
      "metadata": {
        "id": "NH0JIcn0HAPN"
      },
      "id": "NH0JIcn0HAPN"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Merges two PubMedQA JSON datasets into one, adding ground truth labels.\n",
        "\"\"\"\n",
        "file1_path = \"NLP_Project/Pubmedqa_data/ori_pqal/ori_pqal.json\"\n",
        "file2_path = \"NLP_Project/Pubmedqa_data/ori_pqal/test_ground_truth.json\"\n",
        "output_path = \"NLP_Project/Pubmedqa_data/ori_pqal.json\"\n",
        "\n",
        "with open(file1_path, \"r\") as f:\n",
        "    file1_data = json.load(f)\n",
        "with open(file2_path, \"r\") as f:\n",
        "    file2_data = json.load(f)\n",
        "\n",
        "for key in file1_data:\n",
        "    file1_data[key][\"ground_truth\"] = file2_data.get(key, \"None\")\n",
        "\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(file1_data, f, indent=4)\n",
        "\n",
        "print(f\"Merged dataset saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "D5aKf25iG-ZW"
      },
      "id": "D5aKf25iG-ZW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract and display PubMedQA fields\n"
      ],
      "metadata": {
        "id": "zPOF5RqAHFPJ"
      },
      "id": "zPOF5RqAHFPJ"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extracts and displays 'QUESTION' and 'ground_truth' fields from the merged PubMedQA dataset.\n",
        "\"\"\"\n",
        "input_path = \"NLP_Project/Pubmedqa_data/ori_pqal.json\"\n",
        "with open(input_path, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "extracted_data = {\n",
        "    key: {\"QUESTION\": value[\"QUESTION\"], \"ground_truth\": value[\"ground_truth\"]}\n",
        "    for key, value in data.items()\n",
        "    if \"QUESTION\" in value and \"ground_truth\" in value\n",
        "}\n",
        "\n",
        "for key, item in extracted_data.items():\n",
        "    print(f\"{key}:\")\n",
        "    print(f\"  QUESTION: {item['QUESTION']}\")\n",
        "    print(f\"  ground_truth: {item['ground_truth']}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "B9o4c0KUHCSX"
      },
      "id": "B9o4c0KUHCSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and display datasets\n"
      ],
      "metadata": {
        "id": "zE2ycBzmHKPg"
      },
      "id": "zE2ycBzmHKPg"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loads and displays the first few rows of MedQA-USMLE, Medical Meadow, and PubMedQA datasets.\n",
        "\"\"\"\n",
        "medqa_train_path = \"NLP_Project/MedQA_USMLE/train/medqa_usmle_train.csv\"\n",
        "medqa_test_path = \"NLP_Project/MedQA_USMLE/test/medqa_usmle_test.csv\"\n",
        "medical_meadow_path = \"NLP_Project/Medical_Meadow/train/medical_meadow_train.csv\"\n",
        "pubmedqa_ori_pqau_path = \"NLP_Project/Pubmedqa_data/ori_pqau.json\"\n",
        "\n",
        "medqa_train = pd.read_csv(medqa_train_path)\n",
        "print(\"MedQA-USMLE Train Dataset:\")\n",
        "print(medqa_train.head())\n",
        "\n",
        "medqa_test = pd.read_csv(medqa_test_path)\n",
        "print(\"\\nMedQA-USMLE Test Dataset:\")\n",
        "print(medqa_test.head())\n",
        "\n",
        "medical_meadow = pd.read_csv(medical_meadow_path)\n",
        "print(\"\\nMedical Meadow Train Dataset:\")\n",
        "print(medical_meadow.head())\n",
        "\n",
        "with open(pubmedqa_ori_pqau_path, \"r\") as f:\n",
        "    pubmedqa_data = json.load(f)\n",
        "print(\"\\nPubMedQA ori_pqau.json First Entry:\")\n",
        "first_key = list(pubmedqa_data.keys())[0]\n",
        "print(json.dumps(pubmedqa_data[first_key], indent=4))\n"
      ],
      "metadata": {
        "id": "Otwx1Oe3HIWn"
      },
      "id": "Otwx1Oe3HIWn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define text cleaning functions\n"
      ],
      "metadata": {
        "id": "Z3laBiWGHOn2"
      },
      "id": "Z3laBiWGHOn2"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Defines functions for general text cleaning, dictionary cleaning, and list cleaning.\n",
        "\"\"\"\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Lowercase, remove special characters, and normalize whitespace.\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
        "    return text.strip() if text.strip() else \"None\"\n",
        "\n",
        "def clean_dict(data_dict):\n",
        "    \"\"\"Clean each value in a dictionary using clean_text.\"\"\"\n",
        "    return {key: clean_text(value) for key, value in data_dict.items()}\n",
        "\n",
        "def clean_list(data_list):\n",
        "    \"\"\"Clean each item in a list using clean_text.\"\"\"\n",
        "    return [clean_text(item) for item in data_list]\n"
      ],
      "metadata": {
        "id": "2SEBORqIHNXW"
      },
      "id": "2SEBORqIHNXW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess MedQA-USMLE dataset\n"
      ],
      "metadata": {
        "id": "wPjWGZUmHT8Y"
      },
      "id": "wPjWGZUmHT8Y"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies text cleaning to fields in the MedQA-USMLE dataset and saves the preprocessed data.\n",
        "\"\"\"\n",
        "medqa_path = \"NLP_Project/MedQA_USMLE/MedQA_USMLE_dataset.csv\"\n",
        "medqa_df = pd.read_csv(medqa_path)\n",
        "\n",
        "medqa_df['question'] = medqa_df['question'].apply(clean_text)\n",
        "medqa_df['options'] = medqa_df['options'].apply(clean_dict)\n",
        "medqa_df['meta_info'] = medqa_df['meta_info'].apply(clean_text)\n",
        "\n",
        "print(\"\\nAfter Preprocessing:\")\n",
        "print(medqa_df[['question', 'options', 'answer', 'answer_idx']].head())\n",
        "\n",
        "output_path = \"NLP_Project/Preprocessed/medqa_usmle_preprocessed.csv\"\n",
        "medqa_df.to_csv(output_path, index=False)\n",
        "print(f\"\\nPreprocessed MedQA-USMLE dataset saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "95HnsVGyHRf-"
      },
      "id": "95HnsVGyHRf-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Medical Meadow dataset\n"
      ],
      "metadata": {
        "id": "WviRH6WCHXY3"
      },
      "id": "WviRH6WCHXY3"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies text cleaning to fields in the Medical Meadow dataset and saves the preprocessed data.\n",
        "\"\"\"\n",
        "medical_meadow_df = pd.read_csv(medical_meadow_path)\n",
        "\n",
        "medical_meadow_df['input'] = medical_meadow_df['input'].apply(clean_text)\n",
        "medical_meadow_df['output'] = medical_meadow_df['output'].apply(clean_text)\n",
        "\n",
        "if 'instruction' in medical_meadow_df.columns:\n",
        "    medical_meadow_df['instruction'] = medical_meadow_df['instruction'].apply(clean_text)\n",
        "\n",
        "output_path = \"NLP_Project/Preprocessed/medical_meadow_preprocessed.csv\"\n",
        "medical_meadow_df.to_csv(output_path, index=False)\n",
        "print(f\"\\nPreprocessed Medical Meadow dataset saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "3-yfuayrHVzN"
      },
      "id": "3-yfuayrHVzN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess PubMedQA dataset\n"
      ],
      "metadata": {
        "id": "V4J_9yHwHVke"
      },
      "id": "V4J_9yHwHVke"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies text cleaning to all relevant fields in the PubMedQA dataset and saves the preprocessed data.\n",
        "\"\"\"\n",
        "pubmedqa_path = \"NLP_Project/Pubmedqa_data/Pubmedqa_data.json\"\n",
        "with open(pubmedqa_path, \"r\") as f:\n",
        "    pubmedqa_data = json.load(f)\n",
        "\n",
        "for key, entry in pubmedqa_data.items():\n",
        "    entry['QUESTION'] = clean_text(entry.get('QUESTION'))\n",
        "    entry['CONTEXTS'] = clean_list(entry.get('CONTEXTS'))\n",
        "    entry['LONG_ANSWER'] = clean_text(entry.get('LONG_ANSWER'))\n",
        "    entry['LABELS'] = clean_list(entry.get('LABELS'))\n",
        "    entry['MESHES'] = clean_list(entry.get('MESHES'))\n",
        "    entry['YEAR'] = clean_text(entry.get('YEAR'))\n",
        "    entry['reasoning_required_pred'] = clean_text(entry.get('reasoning_required_pred'))\n",
        "    entry['reasoning_free_pred'] = clean_text(entry.get('reasoning_free_pred'))\n",
        "    entry['final_decision'] = clean_text(entry.get('final_decision'))\n",
        "    entry['ground_truth'] = clean_text(entry.get('ground_truth'))\n",
        "\n",
        "preprocessed_dir = \"NLP_Project/Preprocessed\"\n",
        "os.makedirs(preprocessed_dir, exist_ok=True)\n",
        "\n",
        "output_path = os.path.join(preprocessed_dir, \"pubmedqa_preprocessed.json\")\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(pubmedqa_data, f, indent=4)\n",
        "\n",
        "print(f\"\\nPreprocessed PubMedQA dataset saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "4NMkA_H6Hatu"
      },
      "id": "4NMkA_H6Hatu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlpenv",
      "language": "python",
      "name": "nlpenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}