{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries\n"
      ],
      "metadata": {
        "id": "saWyVJh5Is16"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jMv3Un-GiBX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Imports all required libraries for data processing, visualization, and natural language processing.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Sentence-BERT model\n"
      ],
      "metadata": {
        "id": "3OKAZw8-IxQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loads the Sentence-BERT model for semantic similarity tasks.\n",
        "The model used is 'paraphrase-mpnet-base-v2'.\n",
        "\"\"\"\n",
        "print(\"Loading Sentence-BERT model...\")\n",
        "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
        "print(\"Model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "oNI3u10YIvw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load preprocessed datasets\n"
      ],
      "metadata": {
        "id": "Et6--HJ5I14B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loads the preprocessed datasets: MedQA-USMLE, Medical Meadow, and PubMedQA.\n",
        "Combines all contexts for further processing.\n",
        "\"\"\"\n",
        "medqa_path = \"NLP_Project/Preprocessed/medqa_usmle_preprocessed.csv\"\n",
        "medqa_df = pd.read_csv(medqa_path).head(100)  # Load only the first 100 questions for testing\n",
        "\n",
        "medical_meadow_path = \"NLP_Project/Preprocessed/medical_meadow_preprocessed.csv\"\n",
        "medical_meadow_df = pd.read_csv(medical_meadow_path)\n",
        "medical_meadow_contexts = medical_meadow_df['output'].tolist()\n",
        "\n",
        "pubmedqa_path = \"NLP_Project/Preprocessed/pubmedqa_preprocessed.json\"\n",
        "with open(pubmedqa_path, \"r\") as f:\n",
        "    pubmedqa_data = json.load(f)\n",
        "pubmedqa_contexts = [context for entry in pubmedqa_data.values() for context in entry.get('CONTEXTS', [])]\n",
        "\n",
        "# Combine all contexts\n",
        "combined_contexts = medical_meadow_contexts + pubmedqa_contexts\n",
        "context_embeddings = model.encode(combined_contexts, convert_to_tensor=True)\n"
      ],
      "metadata": {
        "id": "23LF8JuII0pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define context retrieval function"
      ],
      "metadata": {
        "id": "2l3ms4HsI6sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Retrieves distinct contexts for each question option based on semantic similarity.\n",
        "\n",
        "Args:\n",
        "    question (str): The main question text.\n",
        "    options (dict): Dictionary of options (e.g., {\"A\": \"Option A text\", ...}).\n",
        "    top_n (int, optional): Number of top contexts to retrieve. Defaults to 1.\n",
        "    token_limit (int, optional): Maximum number of tokens in retrieved contexts. Defaults to 700.\n",
        "\n",
        "Returns:\n",
        "    dict: A dictionary mapping each option to its most relevant context and similarity score.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def retrieve_contexts_with_scores(question, options, top_n=1, token_limit=700):\n",
        "    retrieved_contexts = {}\n",
        "    used_indices = set()\n",
        "\n",
        "    for option_key, option_text in options.items():\n",
        "        query = f\"{question} {option_text} {' '.join(option_text.split() * 2)}\"\n",
        "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "        scores = util.cos_sim(query_embedding, context_embeddings)[0]\n",
        "        scores = scores.cpu().numpy()\n",
        "\n",
        "        filtered_indices = [i for i, score in enumerate(scores) if score > 0.4]\n",
        "\n",
        "        if filtered_indices:                # used guru 99 concept\n",
        "            sorted_indices = np.argsort(scores)[::-1]\n",
        "            for idx in sorted_indices:\n",
        "                if idx in filtered_indices and idx not in used_indices:\n",
        "                    context = combined_contexts[idx]\n",
        "                    similarity = scores[idx]\n",
        "                    truncated_context = ' '.join(context.split()[:token_limit])\n",
        "                    used_indices.add(idx)\n",
        "                    retrieved_contexts[option_key] = (truncated_context, similarity)\n",
        "                    break\n",
        "        else:\n",
        "            max_index = np.argmax(scores)\n",
        "            context = combined_contexts[max_index]\n",
        "            similarity = scores[max_index]\n",
        "            truncated_context = ' '.join(context.split()[:token_limit])\n",
        "            retrieved_contexts[option_key] = (truncated_context + \" (Fallback)\", similarity)\n",
        "\n",
        "    return retrieved_contexts\n"
      ],
      "metadata": {
        "id": "v6mFODb4I48p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process questions and retrieve contexts"
      ],
      "metadata": {
        "id": "3yDSNDIeI_p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Processes each question from the MedQA dataset, retrieves relevant contexts\n",
        "for each option, and stores the results in a structured format.\n",
        "\"\"\"\n",
        "results = []\n",
        "\n",
        "for idx, row in medqa_df.iterrows():\n",
        "    question = row['question']\n",
        "    options = eval(row['options'])  # Convert stringified dict to actual dict\n",
        "\n",
        "    print(f\"\\nProcessing Question {idx+1}:\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"Options:\")\n",
        "    for option_key, option_text in options.items():\n",
        "        print(f\"  {option_key}: {option_text}\")\n",
        "    # used chat gpt for the proper way formation to store in csv\n",
        "    retrieved_contexts = retrieve_contexts_with_scores(question, options)\n",
        "    result_row = {\n",
        "        \"question\": question,\n",
        "        \"option_a\": options.get(\"A\", \"\"),\n",
        "        \"context_a\": retrieved_contexts.get(\"A\", (\"No relevant context found.\", 0))[0],\n",
        "        \"similarity_a\": retrieved_contexts.get(\"A\", (\"\", 0))[1],\n",
        "        \"option_b\": options.get(\"B\", \"\"),\n",
        "        \"context_b\": retrieved_contexts.get(\"B\", (\"No relevant context found.\", 0))[0],\n",
        "        \"similarity_b\": retrieved_contexts.get(\"B\", (\"\", 0))[1],\n",
        "        \"option_c\": options.get(\"C\", \"\"),\n",
        "        \"context_c\": retrieved_contexts.get(\"C\", (\"No relevant context found.\", 0))[0],\n",
        "        \"similarity_c\": retrieved_contexts.get(\"C\", (\"\", 0))[1],\n",
        "        \"option_d\": options.get(\"D\", \"\"),\n",
        "        \"context_d\": retrieved_contexts.get(\"D\", (\"No relevant context found.\", 0))[0],\n",
        "        \"similarity_d\": retrieved_contexts.get(\"D\", (\"\", 0))[1],\n",
        "    }\n",
        "    results.append(result_row)\n"
      ],
      "metadata": {
        "id": "Iwm0f3IWI9ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save results to CSV"
      ],
      "metadata": {
        "id": "aJkLPT6_JEMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Saves the retrieved contexts and similarity scores to a CSV file for further analysis.\n",
        "\n",
        "Output:\n",
        "    CSV file containing questions, options, contexts, and similarity scores.\n",
        "\"\"\"\n",
        "output_path = \"NLP_Project/Retrieved/TRY/Sentence_BERT1.csv\"\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(output_path, index=False)\n",
        "print(f\"Results saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "d8SPbNvsJC1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}