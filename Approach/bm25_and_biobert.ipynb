{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "QxJmT6twL-_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1JBvFk7L4sr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Imports libraries for data processing, BM25 retrieval, BioBERT embedding generation,\n",
        "and hybrid retrieval tasks.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from rank_bm25 import BM25Okapi\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load BioBERT model\n"
      ],
      "metadata": {
        "id": "rouCUFr-MDR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loads the BioBERT model for generating contextual embeddings from biomedical texts.\n",
        "The model used is 'dmis-lab/biobert-base-cased-v1.1'.\n",
        "\"\"\"\n",
        "print(\"Loading BioBERT model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "biobert_model = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\").to(\"cuda\")  # Move model to GPU\n",
        "print(\"BioBERT loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "DUqKI7_CMBQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize BM25 and BioBERT embeddings\n"
      ],
      "metadata": {
        "id": "loTAUYlzMHi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initializes BM25 for term-based retrieval and generates BioBERT embeddings for all contexts.\n",
        "\n",
        "Args:\n",
        "    contexts (list): List of context strings to index with BM25 and embed with BioBERT.\n",
        "\n",
        "Returns:\n",
        "    tuple: BM25 instance and tensor of BioBERT embeddings.\n",
        "\"\"\"\n",
        "def initialize_bm25_and_embeddings(contexts):\n",
        "    print(\"Initializing BM25...\")\n",
        "    bm25 = BM25Okapi([doc.split() for doc in contexts])\n",
        "    print(\"BM25 initialized!\")\n",
        "\n",
        "    print(\"Generating BioBERT embeddings for contexts...\")\n",
        "    embeddings = []\n",
        "    # used stackoverflow\n",
        "    batch_size = 16  # Batch size for efficient memory usage\n",
        "    for i in range(0, len(contexts), batch_size):\n",
        "        batch_texts = contexts[i:i + batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            outputs = biobert_model(**inputs)\n",
        "        embeddings.append(outputs.last_hidden_state[:, 0, :].cpu())  # Use CLS token embeddings\n",
        "        torch.cuda.empty_cache()  # Clear GPU memory\n",
        "    print(\"BioBERT embeddings generated!\")\n",
        "    return bm25, torch.cat(embeddings, dim=0)\n"
      ],
      "metadata": {
        "id": "qrw7WcpJME1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load datasets\n"
      ],
      "metadata": {
        "id": "SKCmDDbSMTuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Loads MedQA-USMLE, Medical Meadow, and PubMedQA datasets, and combines their contexts.\n",
        "\n",
        "Returns:\n",
        "    tuple: DataFrame for MedQA and a combined list of contexts.\n",
        "\"\"\"\n",
        "def load_datasets():\n",
        "    medqa_path = \"NLP_Project/Preprocessed/medqa_usmle_preprocessed.csv\"\n",
        "    medical_meadow_path = \"NLP_Project/Preprocessed/medical_meadow_preprocessed.csv\"\n",
        "    pubmedqa_path = \"NLP_Project/Preprocessed/pubmedqa_preprocessed.json\"\n",
        "\n",
        "    medqa_df = pd.read_csv(medqa_path).head(100)  # Load subset for testing\n",
        "    medical_meadow_df = pd.read_csv(medical_meadow_path)\n",
        "    with open(pubmedqa_path, \"r\") as f:\n",
        "        pubmedqa_data = json.load(f)\n",
        "\n",
        "    medical_meadow_contexts = medical_meadow_df['output'].tolist()\n",
        "    pubmedqa_contexts = [context for entry in pubmedqa_data.values() for context in entry.get('CONTEXTS', [])]\n",
        "\n",
        "    combined_contexts = medical_meadow_contexts + pubmedqa_contexts\n",
        "    print(f\"Total contexts: {len(combined_contexts)}\")\n",
        "    return medqa_df, combined_contexts\n"
      ],
      "metadata": {
        "id": "eoLT_VkcMJjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid retrieval function\n"
      ],
      "metadata": {
        "id": "MGc01cPNMYbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Retrieves top-k contexts for each option using BM25 and ranks them with BioBERT.\n",
        "\n",
        "Args:\n",
        "    question (str): Question text.\n",
        "    options (dict): Dictionary of options (e.g., {\"A\": \"Option A text\", ...}).\n",
        "    bm25 (BM25Okapi): BM25 instance for term-based retrieval.\n",
        "    context_embeddings (torch.Tensor): Precomputed BioBERT embeddings for all contexts.\n",
        "    combined_contexts (list): List of context strings.\n",
        "    top_k (int, optional): Number of top contexts to retrieve. Defaults to 3.\n",
        "    token_limit (int, optional): Maximum token length for retrieved contexts. Defaults to 700.\n",
        "\n",
        "Returns:\n",
        "    tuple: Retrieved contexts and their similarity scores.\n",
        "\"\"\"\n",
        "def hybrid_retrieve_contexts(question, options, bm25, context_embeddings, combined_contexts, top_k=3, token_limit=700):\n",
        "    retrieved_contexts = {}\n",
        "    similarity_scores = {}\n",
        "\n",
        "    for option_key, option_text in options.items():\n",
        "        query = f\"{question} {option_text}\"\n",
        "\n",
        "        # BM25 retrieval\n",
        "        bm25_scores = bm25.get_scores(query.split())\n",
        "        bm25_top_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:50]\n",
        "        bm25_top_contexts = [combined_contexts[i] for i in bm25_top_indices]\n",
        "\n",
        "        # BioBERT ranking\n",
        "        bm25_top_embeddings = torch.stack([context_embeddings[i] for i in bm25_top_indices])\n",
        "        inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            query_embedding = biobert_model(**inputs).last_hidden_state[:, 0, :].cpu()\n",
        "        cosine_similarities = torch.nn.functional.cosine_similarity(query_embedding, bm25_top_embeddings, dim=1)\n",
        "\n",
        "        # Select top context and score\n",
        "        top_index = torch.argmax(cosine_similarities).item()\n",
        "        context = bm25_top_contexts[top_index]\n",
        "        similarity_score = cosine_similarities[top_index].item()\n",
        "        truncated_context = ' '.join(context.split()[:token_limit])\n",
        "\n",
        "        retrieved_contexts[option_key] = truncated_context\n",
        "        similarity_scores[option_key] = similarity_score\n",
        "\n",
        "    return retrieved_contexts, similarity_scores\n"
      ],
      "metadata": {
        "id": "uESEuYO5MW2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process questions\n"
      ],
      "metadata": {
        "id": "Bb7-XxeYMdhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Processes each question in the MedQA dataset and retrieves relevant contexts.\n",
        "\n",
        "Args:\n",
        "    medqa_df (pd.DataFrame): DataFrame containing MedQA questions and options.\n",
        "    bm25 (BM25Okapi): BM25 instance for term-based retrieval.\n",
        "    context_embeddings (torch.Tensor): Precomputed BioBERT embeddings for all contexts.\n",
        "    combined_contexts (list): List of context strings.\n",
        "\n",
        "Returns:\n",
        "    list: Results containing questions, options, retrieved contexts, and similarity scores.\n",
        "\"\"\"\n",
        "def process_questions(medqa_df, bm25, context_embeddings, combined_contexts):\n",
        "    results = []\n",
        "    for idx, row in medqa_df.iterrows():\n",
        "        question = row['question']\n",
        "        options = eval(row['options'])\n",
        "        print(f\"Processing Question {idx+1}: {question}\")\n",
        "        retrieved_contexts, similarity_scores = hybrid_retrieve_contexts(question, options, bm25, context_embeddings, combined_contexts)\n",
        "\n",
        "        result_row = {\n",
        "            \"question\": question,\n",
        "            \"option_a\": options.get(\"A\", \"\"),\n",
        "            \"context_a\": retrieved_contexts.get(\"A\", \"No relevant context found.\"),\n",
        "            \"similarity_a\": similarity_scores.get(\"A\", \"N/A\"),\n",
        "            \"option_b\": options.get(\"B\", \"\"),\n",
        "            \"context_b\": retrieved_contexts.get(\"B\", \"No relevant context found.\"),\n",
        "            \"similarity_b\": similarity_scores.get(\"B\", \"N/A\"),\n",
        "            \"option_c\": options.get(\"C\", \"\"),\n",
        "            \"context_c\": retrieved_contexts.get(\"C\", \"No relevant context found.\"),\n",
        "            \"similarity_c\": similarity_scores.get(\"C\", \"N/A\"),\n",
        "            \"option_d\": options.get(\"D\", \"\"),\n",
        "            \"context_d\": retrieved_contexts.get(\"D\", \"No relevant context found.\"),\n",
        "            \"similarity_d\": similarity_scores.get(\"D\", \"N/A\"),\n",
        "        }\n",
        "        results.append(result_row)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "QlX-bIRuMcB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save results\n"
      ],
      "metadata": {
        "id": "Z0g86J1OMhuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Saves the retrieved results to a CSV file.\n",
        "\n",
        "Args:\n",
        "    results (list): List of results containing questions, options, contexts, and similarity scores.\n",
        "    output_path (str, optional): File path to save the results. Defaults to 'NLP_Project/Retrieved/TRY/hy_bm_bio1.csv'.\n",
        "\"\"\"\n",
        "def save_results(results, output_path=\"NLP_Project/Retrieved/TRY/hy_bm_bio1.csv\"):\n",
        "    pd.DataFrame(results).to_csv(output_path, index=False)\n",
        "    print(f\"Results saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "-cylegv3Mghv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main workflow"
      ],
      "metadata": {
        "id": "x0cwD6xMMnDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Main script to load datasets, initialize retrieval models, process questions,\n",
        "and save the results.\n",
        "\"\"\"\n",
        "if __name__ == \"__main__\":\n",
        "    # Load datasets\n",
        "    medqa_df, combined_contexts = load_datasets()\n",
        "\n",
        "    # Initialize BM25 and generate BioBERT embeddings\n",
        "    bm25, context_embeddings = initialize_bm25_and_embeddings(combined_contexts)\n",
        "\n",
        "    # Process questions and retrieve contexts\n",
        "    results = process_questions(medqa_df, bm25, context_embeddings, combined_contexts)\n",
        "\n",
        "    # Save the results\n",
        "    save_results(results)\n"
      ],
      "metadata": {
        "id": "k2GyoIpyMkLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}